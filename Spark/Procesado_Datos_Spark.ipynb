{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips=spark.read.csv('datasets/NYC_taxi_trip_records',header=True,\n",
    "                       inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.createOrReplaceTempView('viajes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trips.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "|       2| 2018-11-22 22:37:46|  2018-11-22 22:43:06|              2|         1.26|         1|                 N|          43|         237|           2|        6.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.8|\n",
      "|       2| 2018-11-22 22:51:04|  2018-11-22 23:44:18|              3|        21.71|         1|                 N|         236|         262|           1|       62.5|  0.5|    0.5|      10.0|         0.0|                  0.3|        73.8|\n",
      "|       1| 2018-11-22 22:12:05|  2018-11-22 22:26:43|              2|          1.4|         1|                 N|         161|         164|           2|       10.5|  0.5|    0.5|       0.0|         0.0|                  0.3|        11.8|\n",
      "|       1| 2018-11-22 22:36:05|  2018-11-22 22:44:04|              2|          2.3|         1|                 N|         113|         230|           1|        9.0|  0.5|    0.5|      2.05|         0.0|                  0.3|       12.35|\n",
      "|       1| 2018-11-22 22:46:39|  2018-11-22 22:59:38|              1|          3.4|         1|                 N|         230|         193|           1|       13.0|  0.5|    0.5|      2.85|         0.0|                  0.3|       17.15|\n",
      "|       2| 2018-11-22 22:23:25|  2018-11-22 23:03:56|              1|         19.5|         2|                 N|         132|         166|           1|       52.0|  0.0|    0.5|     11.71|        5.76|                  0.3|       70.27|\n",
      "|       2| 2018-11-22 22:38:58|  2018-11-22 23:14:30|              1|        17.36|         1|                 N|         132|         182|           2|       47.5|  0.5|    0.5|       0.0|        5.76|                  0.3|       54.56|\n",
      "|       2| 2018-11-22 22:01:37|  2018-11-22 22:12:20|              2|          1.0|         1|                 N|         100|         170|           1|        8.0|  0.5|    0.5|      1.86|         0.0|                  0.3|       11.16|\n",
      "|       2| 2018-11-22 22:15:02|  2018-11-22 22:22:54|              2|         2.04|         1|                 N|         162|         141|           1|        8.5|  0.5|    0.5|      2.45|         0.0|                  0.3|       12.25|\n",
      "|       2| 2018-11-22 22:26:52|  2018-11-22 22:33:46|              2|         2.52|         1|                 N|         141|          75|           2|        9.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        10.3|\n",
      "|       2| 2018-11-22 22:48:02|  2018-11-22 22:57:53|              2|         1.69|         1|                 N|         170|         246|           2|        8.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         9.8|\n",
      "|       2| 2018-11-22 22:02:41|  2018-11-22 22:09:47|              2|         1.17|         1|                 N|         230|         142|           1|        7.0|  0.5|    0.5|      1.31|         0.0|                  0.3|        9.61|\n",
      "|       2| 2018-11-22 21:59:50|  2018-11-22 22:02:18|              2|         0.43|         1|                 N|         234|         164|           2|        4.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.3|\n",
      "|       2| 2018-11-22 22:07:36|  2018-11-22 22:15:52|              2|         1.58|         1|                 N|         170|         113|           1|        8.0|  0.5|    0.5|      2.32|         0.0|                  0.3|       11.62|\n",
      "|       2| 2018-11-22 22:23:04|  2018-11-22 22:26:19|              2|         0.42|         1|                 N|         114|         113|           2|        4.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         5.3|\n",
      "|       1| 2018-11-22 22:02:18|  2018-11-22 22:08:04|              4|          1.1|         1|                 N|         164|         161|           2|        6.0|  0.5|    0.5|       0.0|         0.0|                  0.3|         7.3|\n",
      "|       1| 2018-11-22 22:10:39|  2018-11-22 22:13:55|              3|          0.4|         1|                 N|         161|          48|           1|        4.5|  0.5|    0.5|       2.9|         0.0|                  0.3|         8.7|\n",
      "|       1| 2018-11-22 22:19:12|  2018-11-22 22:21:41|              3|          0.3|         1|                 N|         230|         230|           2|        3.5|  0.5|    0.5|       0.0|         0.0|                  0.3|         4.8|\n",
      "|       1| 2018-11-22 22:29:22|  2018-11-22 22:42:18|              4|          4.0|         1|                 N|         230|         226|           2|       14.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        15.3|\n",
      "|       1| 2018-11-22 22:56:33|  2018-11-22 23:07:28|              1|          2.1|         1|                 N|         237|          48|           2|       10.0|  0.5|    0.5|       0.0|         0.0|                  0.3|        11.3|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM trips').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, tpep_pickup_datetime: timestamp, tpep_dropoff_datetime: timestamp, passenger_count: int, trip_distance: double, RatecodeID: int, store_and_fwd_flag: string, PULocationID: int, DOLocationID: int, payment_type: int, fare_amount: double, extra: double, mta_tax: double, tip_amount: double, tolls_amount: double, improvement_surcharge: double, total_amount: double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Contar o número de viaxes entre 00:00 e 01:00 de cada día e dar o total por día (agrupados por día, tódolos días teñen un reconto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      data|count|\n",
      "+----------+-----+\n",
      "|2008-12-31|    3|\n",
      "|2009-01-01|   25|\n",
      "|2018-10-31|  169|\n",
      "|2018-11-01|10356|\n",
      "|2018-11-02|10735|\n",
      "|2018-11-03|14348|\n",
      "|2018-11-04|12737|\n",
      "|2018-11-05| 4582|\n",
      "|2018-11-06| 5603|\n",
      "|2018-11-07| 7296|\n",
      "|2018-11-08| 9171|\n",
      "|2018-11-09|12400|\n",
      "|2018-11-10|15177|\n",
      "|2018-11-11|13227|\n",
      "|2018-11-12| 5100|\n",
      "|2018-11-13| 6282|\n",
      "|2018-11-14| 7493|\n",
      "|2018-11-15| 8978|\n",
      "|2018-11-16|10524|\n",
      "|2018-11-17|14927|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, date_format\n",
    "from pyspark.sql import SparkSession\n",
    "df_filtrado = df.where((date_format(col(\"tpep_pickup_datetime\"), \"HH:mm:ss\") >= \"00:00:00\") &\n",
    "    (date_format(col(\"tpep_dropoff_datetime\"), \"HH:mm:ss\") < \"01:00:00\")).groupBy(date_format(\"tpep_pickup_datetime\", \"yyyy-MM-dd\").alias(\"data\")).count().orderBy(\"data\")\n",
    "df_filtrado.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Contar o número de viaxes entre 00:00 e 01:00 de cada día e dar o total por mes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|    mes|sum(count)|\n",
      "+-------+----------+\n",
      "|2008-12|         3|\n",
      "|2009-01|        25|\n",
      "|2018-10|       169|\n",
      "|2018-11|    269844|\n",
      "|2018-12|    298290|\n",
      "|2019-01|        26|\n",
      "|2019-02|         1|\n",
      "|2019-03|         1|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, date_format\n",
    "from pyspark.sql import SparkSession\n",
    "df_filtrado2= df_filtrado.groupBy(date_format(\"data\", \"yyyy-MM\").alias(\"mes\")).sum(\"count\").orderBy(\"mes\")\n",
    "\n",
    "df_filtrado2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) A media de viaxes ao mes que fai cada conductor (VendorID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------------+\n",
      "|VendorID|    mes|media_viajes_mes|\n",
      "+--------+-------+----------------+\n",
      "|       2|2018-11|       4751768.0|\n",
      "|       2|2018-10|           269.0|\n",
      "|       2|2009-01|            85.0|\n",
      "|       2|2019-05|             1.0|\n",
      "|       2|2019-01|            38.0|\n",
      "|       4|2018-11|        130817.0|\n",
      "|       1|2018-12|       3209235.0|\n",
      "|       2|2019-02|             6.0|\n",
      "|       4|2018-12|        112210.0|\n",
      "|       1|2018-11|       3262928.0|\n",
      "|       2|2003-12|             1.0|\n",
      "|       2|2018-12|       4850961.0|\n",
      "|       2|2019-11|             6.0|\n",
      "|       2|2020-12|             2.0|\n",
      "|       2|2008-12|            49.0|\n",
      "|       2|2019-06|            15.0|\n",
      "|       2|2019-03|             4.0|\n",
      "+--------+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_c=spark.sql('''\n",
    "    SELECT \n",
    "        VendorID,\n",
    "        DATE_FORMAT(tpep_pickup_datetime, 'yyyy-MM') AS mes,\n",
    "        AVG(COUNT(*)) OVER (PARTITION BY VendorID, DATE_FORMAT(tpep_pickup_datetime, 'yyyy-MM')) AS media_viajes_mes\n",
    "    FROM \n",
    "        trips\n",
    "    GROUP BY \n",
    "        VendorID, DATE_FORMAT(tpep_pickup_datetime, 'yyyy-MM')\n",
    "''')\n",
    "df_c.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) A media de viaxes ao día que fai cada conductor (VendorID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------------+\n",
      "|VendorID|       mes|media_viajes_mes|\n",
      "+--------+----------+----------------+\n",
      "|       4|2018-11-19|          3800.0|\n",
      "|       4|2018-12-21|          3836.0|\n",
      "|       1|2018-12-19|        120722.0|\n",
      "|       2|2018-11-06|        162115.0|\n",
      "|       2|2018-12-09|        155120.0|\n",
      "|       1|2018-11-30|        124868.0|\n",
      "|       2|2019-01-12|             3.0|\n",
      "|       2|2008-12-31|            49.0|\n",
      "|       1|2018-11-04|         97453.0|\n",
      "|       4|2018-11-01|          5117.0|\n",
      "|       4|2018-12-20|          4154.0|\n",
      "|       1|2018-12-22|         96187.0|\n",
      "|       4|2018-12-18|          4028.0|\n",
      "|       4|2018-12-22|          3453.0|\n",
      "|       1|2018-11-05|        110679.0|\n",
      "|       2|2018-11-22|        108013.0|\n",
      "|       2|2020-12-10|             2.0|\n",
      "|       1|2018-11-12|        104009.0|\n",
      "|       2|2019-01-18|             1.0|\n",
      "|       4|2018-11-11|          4450.0|\n",
      "+--------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_c=spark.sql('''\n",
    "    SELECT \n",
    "        VendorID,\n",
    "        DATE_FORMAT(tpep_pickup_datetime, 'yyyy-MM-dd') AS mes,\n",
    "        AVG(COUNT(VendorID)) OVER (PARTITION BY VendorID, DATE_FORMAT(tpep_pickup_datetime, 'yyyy-MM-dd')) AS media_viajes_mes\n",
    "    FROM \n",
    "        trips\n",
    "    GROUP BY \n",
    "        VendorID, DATE_FORMAT(tpep_pickup_datetime, 'yyyy-MM-dd')\n",
    "''')\n",
    "df_c.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) Cantos pasaxeiros (número de persoas na viaxe) foron como máximo na primeira semana do mes (nunha viaxe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|maximo|\n",
      "+------+\n",
      "|     9|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_pas= spark.sql(\"\"\"SELECT MAX(passenger_count) as maximo\n",
    "            FROM trips\n",
    "            WHERE day(tpep_pickup_datetime) <= 7\"\"\")\n",
    "max_pas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F) Cantos pasaxeiros (número de persoas na viaxe) foron como máximo en todo o mes (nunha viaxe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|maximo|mes|\n",
      "+------+---+\n",
      "|     9| 12|\n",
      "|     6|  1|\n",
      "|     1|  6|\n",
      "|     5|  3|\n",
      "|     5|  5|\n",
      "|     6| 10|\n",
      "|    96| 11|\n",
      "|     5|  2|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_pas= spark.sql(\"\"\"SELECT MAX(passenger_count) as maximo, month(tpep_pickup_datetime) as mes\n",
    "            FROM trips\n",
    "            group by month(tpep_pickup_datetime)\n",
    "            \"\"\")\n",
    "max_pas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|mes|maximo|\n",
      "+---+------+\n",
      "| 12|     9|\n",
      "|  1|     6|\n",
      "|  6|     1|\n",
      "|  3|     5|\n",
      "|  5|     5|\n",
      "| 10|     6|\n",
      "| 11|    96|\n",
      "|  2|     5|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import month, max\n",
    "max_pas = df.groupBy(month(\"tpep_pickup_datetime\").alias(\"mes\")).agg(max(\"passenger_count\").alias(\"maximo\"))\n",
    "max_pas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### G) Cantos cartos costou o percorrido máis caro. supoño que e total_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325478.98"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viajes=trips.rdd\n",
    "\n",
    "viajes.max(lambda x:x[-1])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325478.98"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.selectExpr(\"max(total_amount)\").first()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H) Cantos cartos costou o percorrido máis barato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-450.3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viajes.min(lambda x:x[-1])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-450.3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips.selectExpr(\"min(total_amount)\").first()[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
